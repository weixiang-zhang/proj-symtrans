<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>EVOS: Efficient Implicit Neural Training via EVOlutionary Selector</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">EVOS: Efficient Implicit Neural Training via EVOlutionary Selector</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://weixiang-zhang.github.io/" target="_blank">Weixiang Zhang</a><sup>1</sup>,</span>
                <span class="author-block">
                  <a href="https://shuzhaoxie.github.io/" target="_blank">Shuzhao Xie</a><sup>1</sup>,</span>
                  <span class="author-block">
                    Chengwei Ren<sup>1</sup>
                  </span>
                  <span class="author-block">
                    Siyi Xie<sup>2</sup>
                  </span>
                  <span class="author-block">
                    <a href="https://shuzhaoxie.github.io/" target="_blank">Chen Tang</a><sup>3</sup>,</span>
                  </span>
                  <span class="author-block">
                    Shijia Ge<sup>1</sup>
                  </span>
                  <span class="author-block">
                    Mingzi Wang<sup>1</sup>
                  </span>
                  <span class="author-block">
                    <a href="http://pages.mmlab.top/" target="_blank">Zhi Wang</a><sup>1</sup><sup>*</sup>
                  </span>

                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>Tsinghua University; <sup>2</sup>Xi‚Äôan Jiaotong University; <sup>3</sup>The Chinese University of Hong Kong<br>CVPR 2025</span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Corresponding Author</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2412.10153" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <span class="link-block">
                      <a href="static/pdfs/03247_supp.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/zwx-open/EVOS-INR" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/2412.10153" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div class="item">
        <img src="static/images/pipelin6.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          <small>
          <b>Overview of EVOS framework.</b>
          The proposed method aims to optimize a MLP for implicit signal representation via evolutionary selector. 
          The process comprises three key components: (1) <b>(Sparse) Fitness Evaluation</b> for efficiently guiding coordinate selection, 
          (2) <b>(Frequency-Guided) Crossover</b> for improving performance by balancing frequency domain preferences, and 
          (3) <b>(Augmented Unbiased) Mutation</b> for mitigating selection bias in each iteration. 
          The selected coordinates from this evolutionary process are then fed into the network, enabling sparsified forward passes and reduced computational costs.
        </small>
        </h2>
      </div>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">üìå Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We propose <b><u>EVOlutionary Selector (EVOS)</u></b>, an efficient training paradigm for accelerating Implicit Neural Representation (INR). Unlike conventional INR training that feeds all samples through the neural network in each iteration, our approach restricts training to <b>strategically selected points</b>, reducing computational overhead by eliminating redundant forward passes. 
            Specifically, we treat each sample as an individual in an evolutionary process, where only those fittest ones survive and merit inclusion in training, adaptively evolving with the neural network dynamics. 
            While this is conceptually similar to Evolutionary Algorithms, their distinct objectives (selection for acceleration vs. iterative solution optimization) require a fundamental redefinition of evolutionary mechanisms for our context.
            In response, we design sparse fitness evaluation, frequency-guided crossover, and augmented unbiased mutation to comprise EVOS. 
            These components respectively guide sample selection with reduced computational cost, enhance performance through frequency-domain balance, and mitigate selection bias from cached evaluation. 
            Extensive experiments demonstrate that our method achieves approximately 48%-66% reduction in training time while ensuring superior convergence without additional cost, establishing state-of-the-art acceleration among recent sampling-based strategies.
          </p>
        
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item" style="text-align: center;">
        <img src="static/images/exp_1.png" style="width:70%; height:70%;"/>
        <h2 class="subtitle has-text-centered">
          <small>We compared our method against Uniform Sampling (Uniform.), EGRA, Expansive Supervision (Expan.), Soft Mining, INT, <br>
          and conventional full-coordinate training (Standard) commonly used in INR encoding.<br>
          Our method demonstrates consistent superior efficiency across all comparative strategies. <br>
          <i>Strategies without underlines employ constant scheduler, while <u>underlined strategies</u> implement step-wise scheduler.</i></small>
        </h2>
      </div>

      <div class="item" style="text-align: center;">
        <img src="static/images/visual_compare.png" style="width:60%; height:60%;"/>
        <h2 class="subtitle has-text-centered">
          <small>
            Visual comparison of sampling-based acceleration methods for INR training. <br>
            Detailed examination reveals that our method particularly excels in preserving high-fidelity details, <br>
            as evidenced by the wolf's eye and fur regions, producing results closest to ground truth.
        </small>
        </h2>
      </div>

      <div class="item" style="text-align: center;">
        <img src="static/images/evos-loss.png" style="width:60%; height:60%;"/>
        <h2 class="subtitle has-text-centered">
          <small>
            Comparison of reconstruction quality across different sampling-based methods over training time. <br>
            EVOS demonstrates both faster convergence and superior performance compared to alternative approaches. <br>
            Moreover, EVOS exhibits more stable performance gains than INT, maintaining its advantage throughout the training process.
        </small>
        </h2>
      </div>

  </div>
</div>
</div>
</section>



<section class="section hero ">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths" style="border-width: 2px; border-style: solid; border-color: #235d3a; border-radius: 20px;background-color: #F1F9F2;" >
        <h2 class="title is-3">üí° Insights</h2>
        <div class="content has-text-justified">
          <p>
            <b>Sample Weighting ALSO Benefits ‚ÄúOverfitting‚Äù.</b>
            An intuitive assumption suggests that sparsifying training samples would compromise per-iteration reconstruction quality while reducing computational cost, due to incomplete data utilization. 
            However, the results of EVOS (w/o CFS) in our experiment challenge this intuition, demonstrating that our strategy achieves superior fitting quality compared to full-data training under identical iteration counts.
            This phenomenon can be understood through the lens of sample weighting, a technique that improves model generality by adjusting sample observation frequencies during training.
            Indeed, EVOS can be viewed as a specialized form of sample weighting that reweights signal coordinates during signal fitting, implicitly regularizing the loss function through selective sampling.
            Notably, this finding extends the benefits of sample weighting beyond traditional model generalization to signal fitting tasks (inherently an overfitting scenario without test set validation), revealing an intriguing contradiction that merits further investigation.
          </p>
        
        </div>
      </div>
    </div>
  </div>
</section>


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
        @misc{evos,
          title={EVOS: Efficient Implicit Neural Training via EVOlutionary Selector}, 
          author={Weixiang Zhang and Shuzhao Xie and Chengwei Ren and Siyi Xie and Chen Tang and Shijia Ge and Mingzi Wang and Zhi Wang},
          year={2024},
          eprint={2412.10153},
          archivePrefix={arXiv},
          primaryClass={cs.CV},
          url={https://arxiv.org/abs/2412.10153}, 
    }
      </code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the¬†<a href="https://nerfies.github.io" target="_blank">Nerfies</a>¬†project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
